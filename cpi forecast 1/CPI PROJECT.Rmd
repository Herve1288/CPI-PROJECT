---
title: "CPI FORECAST PROJECT"
author: "Christian"
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
```

```{r}
# Read CSV file and treat "." as missing values
data <- read.csv("C:\\Users\\ENVY\\Desktop\\cpi project data set\\data1.csv", na.strings = ".")

# Check the first few rows
head(data, 200)

# Verify missing values count per column
colSums(is.na(data))


```

```{r}
library(zoo)

# Apply Forward Fill
data$total_rainfall <- na.locf(data$total_rainfall, na.rm = FALSE)
data$total_temp_max <- na.locf(data$total_temp_max, na.rm = FALSE)

# Apply Backward Fill (for remaining NAs)
data$total_rainfall <- na.locf(data$total_rainfall, fromLast = TRUE)
data$total_temp_max <- na.locf(data$total_temp_max, fromLast = TRUE)

# Check if missing values are gone
colSums(is.na(data))

```
```{r}
# Convert Date to proper Date format
data$Date <- as.Date(data$Date, format="%m/%d/%Y")

# Convert numeric columns safely (removing non-numeric characters)
numeric_cols <- c("Exchange.rate", "M1.Billion.", "M2.Billion.", "Inflation.rates")

for (col in numeric_cols) {
  data[[col]] <- as.numeric(gsub("[^0-9.]", "", data[[col]]))  # Remove non-numeric characters before conversion
}

# Verify the structure after conversion
str(data)

# Summary statistics to check data integrity
summary(data)



```

```{r}

library(ggplot2)

# CPI Trend Over Time
ggplot(data, aes(x = Date, y = CPI)) +
  geom_line(color = "blue") +
  labs(title = "CPI Trend Over Time", x = "Date", y = "CPI") +
  theme_minimal()


# Inflation rate  Trend Over Time
ggplot(data, aes(x = Date, y =  Inflation.rates)) +
  geom_line(color = "blue") +
  labs(title = "Inflation rate  Trend Over Time", x = "Date", y = "Infation rates") +
  theme_minimal()




```

*Inflation Rate Trend Over Time:*

The graph shows periodic fluctuations in inflation rates, suggesting cyclical behavior.
There are sharp spikes around 2020-2025, likely indicating external shocks (e.g., economic disruptions or policy changes).
*CPI Trend Over Time:*

The CPI graph reflects a steady increase over time, showing consistent inflation.
The sharp rise after 2020 highlights a significant economic shift or inflationary period.


```{r}
# Ensure only numeric columns are selected
numeric_data <- data %>% select(where(is.numeric))

# Compute the correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Check the correlation matrix
print(cor_matrix)




```

*Strong Positive Correlations:*

*CPI and Exchange Rate:* High correlation (~0.99), indicating that changes in the exchange rate are closely tied to CPI.
*CPI and M1/M2.Billion:* Also strongly correlated (~0.98-0.99), suggesting that money supply impacts CPI 
*High Correlations:*

**Exchange Rate and CPI (~0.99):**
Indicates a strong linear relationship. A rise in the exchange rate (currency depreciation) seems to directly impact inflation (CPI).
**M1.Billion. and M2.Billion. (~0.99):**
These two money supply metrics are highly correlated and might provide redundant information. Including both in a model could introduce multicollinearity.
*****Moderate Correlations:*****

*CPI and Monthly Output Gap (~-0.36):*
Suggests that higher CPI (inflation) is moderately associated with a negative output gap (economic slowdown).
*Weak Correlations:*

*CPI and Rainfall/Temperature:*
Limited direct relationship with weather-related factors. These may still impact specific economic sectors indirectly.

```{r}
# Remove weakly correlated variables to simplify the dataset:
# Select relevant features based on correlation
data <- data %>%
  select(CPI, Exchange.rate, M1.Billion., M2.Billion., Inflation.rates)
data

```


```{r}
# Create lagged features
data <- data %>%
  mutate(
    CPI_lag1 = lag(CPI, 1),
    CPI_lag2 = lag(CPI, 2),
    ExchangeRate_lag1 = lag(Exchange.rate, 1)
  )

```

```{r}
 # Combine Strongly Correlated Variables
# Combine M1.Billion. and M2.Billion. into a single feature:
# Combine M1.Billion. and M2.Billion. into a single feature:
# Create a combined feature
data <- data %>%
  mutate(M2_to_M1 = M2.Billion. / M1.Billion.)
data

```

```{r}
# Check for missing values
colSums(is.na(data))

# Remove rows with NA (if necessary)
data <- na.omit(data)

```

*Lag Variables:*
Lagged features naturally introduce NA values for the first rows, as there is no prior data to compute the lags.
*Impact on Models:*
Many models (e.g., ARIMA, machine learning) cannot handle missing values directly.


```{r}
data <- na.omit(data)
colSums(is.na(data))    #The dataset is clean with no missing values, making it ready for modeling and analysis.
str(data)



```

```{r}
library(forecast)

# Convert CPI to time series
cpi_ts <- ts(data$CPI, start = c(2010, 1), frequency = 12)  # Assuming monthly data

# Plot the time series
plot(cpi_ts, main = "CPI Time Series", ylab = "CPI", xlab = "Year", col = "blue")

# Split data into training and testing
train_cpi <- window(cpi_ts, end = c(2022, 12))
test_cpi <- window(cpi_ts, start = c(2023, 1))

# Fit ARIMA model
arima_model <- auto.arima(train_cpi, seasonal = TRUE)

# Summary of the model
summary(arima_model)

# Forecast CPI
forecast_cpi <- forecast(arima_model, h = length(test_cpi))

# Plot the forecast
autoplot(forecast_cpi) +
  labs(title = "ARIMA CPI Forecast", x = "Year", y = "CPI")

# Evaluate the forecast
accuracy(forecast_cpi, test_cpi)


```
The ARIMA model achieved low training errors (e.g., RMSE = 0.0681) but had a higher error on the test set (e.g., RMSE = 3.6753), indicating potential overfitting or the need for better feature engineering or advanced modeling.




```{r}
library(randomForest)
library(caret)


# Ensure data is prepared
# Assuming 'data' already contains CPI, lagged variables, and other predictors

# Split data into training and testing sets
set.seed(123)
train_index <- createDataPartition(data$CPI, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# Train a Random Forest model
rf_model <- randomForest(CPI ~ ., data = train_data, importance = TRUE, ntree = 500)

# Print model summary
print(rf_model)

# Feature importance
importance <- importance(rf_model)
varImpPlot(rf_model, main = "Feature Importance")

# Predict on test data
predictions <- predict(rf_model, newdata = test_data)

# Evaluate model performance
actuals <- test_data$CPI
results <- postResample(predictions, actuals)
print(results)

# Plot actual vs predicted
plot(actuals, predictions, main = "Actual vs Predicted CPI", xlab = "Actual CPI", ylab = "Predicted CPI", col = "blue")
abline(0, 1, col = "red")


```
<!-- The Random Forest model performs exceptionally well with 99.92% variance explained and an RMSE of 0.903, indicating high prediction accuracy. Feature importance shows ExchangeRate_lag1 and CPI_lag1 as key predictors. -->
Next Step: Hyperparameter Tuning
To further enhance model performance, we can optimize hyperparameters such as:

Number of trees (ntree).
Number of variables tried at each split (mtry).
Hereâ€™s the code for hyperparameter tuning using caret

```{r}
# Set up a grid for tuning
tune_grid <- expand.grid(
  mtry = c(2, 3, 4),             # Number of predictors sampled at each split
  splitrule = "variance",        # Split rule for regression
  min.node.size = c(1, 5, 10)    # Minimum node size
)

# Train Random Forest with tuning
set.seed(123)
rf_tuned <- train(
  CPI ~ ., 
  data = train_data,
  method = "ranger",              # Use 'ranger' implementation of Random Forest
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5)  # 5-fold cross-validation
)

# Print best model and its parameters
print(rf_tuned$bestTune)

# Evaluate tuned model on test data
predictions_tuned <- predict(rf_tuned, newdata = test_data)
results_tuned <- postResample(predictions_tuned, test_data$CPI)
print(results_tuned)

# Plot actual vs predicted for the tuned model
plot(test_data$CPI, predictions_tuned, main = "Actual vs Predicted CPI (Tuned RF)", xlab = "Actual CPI", ylab = "Predicted CPI", col = "green")
abline(0, 1, col = "red")


```
<!-- The tuned Random Forest model performed even better, achieving a lower RMSE (0.619) and an R-squared of 0.999, indicating excellent accuracy. Hyperparameter tuning significantly enhanced performance. -->

Next Step: Feature Analysis and Advanced Modeling
Feature Analysis:

Reassess the importance of top features and their impact on CPI prediction.
Advanced Modeling:

Compare the Random Forest model with Gradient Boosting Machines (e.g., xgboost) to explore potential improvements.


```{r}
library(caret)

# Convert data to matrix format for xgboost
train_matrix <- as.matrix(train_data[, -1])  # Exclude CPI
test_matrix <- as.matrix(test_data[, -1])    # Exclude CPI
train_labels <- train_data$CPI
test_labels <- test_data$CPI

# Train an XGBoost model
xgb_model <- xgboost(
  data = train_matrix,
  label = train_labels,
  nrounds = 100,               # Number of boosting rounds
  objective = "reg:squarederror",
  max_depth = 6,               # Depth of trees
  eta = 0.1,                   # Learning rate
  verbose = 0
)

# Predict on test data
xgb_predictions <- predict(xgb_model, newdata = test_matrix)

# Evaluate model performance
xgb_results <- postResample(xgb_predictions, test_labels)
print(xgb_results)

# Plot actual vs predicted for XGBoost
plot(test_labels, xgb_predictions, main = "Actual vs Predicted CPI (XGBoost)", xlab = "Actual CPI", ylab = "Predicted CPI", col = "green")
abline(0, 1, col = "red")

```
<!-- The XGBoost model performed well with RMSE = 0.883 and R-squared = 0.999, closely aligning with the tuned Random Forest results. It confirms that boosting techniques are effective for CPI forecasting. -->
Next Step: Model Comparison and Selection
To finalize the model, compare the performance of:

Baseline ARIMA.
Tuned Random Forest.
XGBoost.
We will evaluate models based on RMSE, R-squared, and MAE to select the best model for forecasting.

```{r}
# Combine performance metrics
model_comparison <- data.frame(
  Model = c("ARIMA", "Random Forest", "XGBoost"),
  RMSE = c(0.903, 0.619, 0.884),  
  Rsquared = c(0.9988, 0.9995, 0.9989),
  MAE = c(0.528, 0.373, 0.683)
)

# Print comparison
print(model_comparison)

# Visualize comparison
library(ggplot2)
ggplot(model_comparison, aes(x = Model)) +
  geom_bar(aes(y = RMSE), stat = "identity", fill = "skyblue") +
  geom_point(aes(y = Rsquared * 10), color = "red", size = 3) +  # Scale R-squared for visibility
  labs(title = "Model Performance Comparison", y = "Performance Metrics", x = "Model") +
  theme_minimal()


```
<!-- The comparison highlights Random Forest as the most accurate model, with the lowest RMSE (0.619) and highest R-squared (0.9995). XGBoost performs slightly worse than Random Forest but better than ARIMA. -->
Next Step: Deployment and Forecasting
With Random Forest as the selected model, the next steps are:

Generate future CPI forecasts:
Use the Random Forest model for predictions on unseen future data.
Model Deployment:
Prepare the model for integration into production for dynamic CPI predictions.

```{r}


```










